==========================================
SLURM_JOB_ID = 5896
SLURM_NODELIST = virya1
==========================================
Fri Dec  1 14:49:10 2023       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  Tesla V100-PCIE-32GB           Off | 00000000:14:00.0 Off |                    0 |
| N/A   29C    P0              26W / 250W |      0MiB / 32768MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
2023-12-01 14:49:18,145 - INFO - Starting script
2023-12-01 14:49:18,209 - INFO - Available device: cuda:0
2023-12-01 14:49:24,650 - INFO - Number of parameters: 453170688
2023-12-01 14:49:24,681 - INFO - Dataset loaded
2023-12-01 14:49:24,683 - INFO - Number of training sequences: 3654
2023-12-01 14:49:24,683 - INFO - Number of training labels: 3654
2023-12-01 14:49:24,683 - INFO - Starting to process training sequences
  0%|          | 0/3654 [00:00<?, ?it/s]2023-12-01 14:49:24,690 - ERROR - Error processing sequence at index 0: MSHEKNEASGNPEAQSWKAQEAMLGVKTEVSRWRAVKNCLYRHLVKVLGEDWIFLLLLGALMALVSWAMDFIGSRGLRFYKYLFAMVEGNLGLQYLVWVCYPLILILFSSLFCQIVSPQAVGSGIPELKTIIRGAVLHEYLTLRTFVAKTVGLTVALSAGFPLGKEGPFVHIASICATLLNQLLCFISGRREEPYYLRADILTVGCALGISCCFGTPLAGVLFSIEVTCSHFGVRSYWRGFLGGAFSAFIFRVLSVWVKDTVTLTALFKTNFRGDIPFDLQELPAFAIIGIASGFFGALFVYLNRQIIVFMRKKNFVTKILKKQRLIYPAVVTFVLATLRFPPGVGQFFGAGLMPRETINSLFDNYTWTKTIDPRGLGNSAQWFIPHLNIFIVMALYFVMHFWMAALAVTMPVPCGAFVPVFNLGAVLGRFVGELMALLFPDGLVSNGNLYHILPGEYAVIGAAAMTGAVTHAVSTAVICFELTGQISHVLPMMVAVILANMVAQGLQPSLYDSIIQIKKLPYLPELSWSSANKYNIQVGDIMVRDVTSIASTSTYGDLLHVLRQTKLKFFPFVDTPETNTLLGSIERTEVEGLLQRRISAYRRQPATAAEAEEEGRNGERGASFTGDVPGEAETSFAYIDQEEAEGQQQREGLEAVKVQTEDPRPPSPVPAEEPTQTSGIYQKKHKGTGQVASRFEEMLTLEEIYQWEQREKNVVVNFETCRIDQSPFQLVEGTSLQKTHTLFSLLGLDRAYVTSMGKLVGVVALAEIQAAIEGSYQKGFRLPPPLASFRDAKNARNSGRTATSNSSGK - Error: PreTokenizedEncodeInput must be Union[PreTokenizedInputSequence, Tuple[PreTokenizedInputSequence, PreTokenizedInputSequence]]
Processed: 0:   0%|          | 0/3654 [00:00<?, ?it/s]Processed: 0:   0%|          | 0/3654 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/h_ghazik/plm_secondary_integration/ionchannel_embedding.py", line 79, in embed_dataset
    ids = tokenizer.batch_encode_plus(
  File "/home/h_ghazik/.conda/envs/py39/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 3067, in batch_encode_plus
    return self._batch_encode_plus(
  File "/home/h_ghazik/.conda/envs/py39/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py", line 504, in _batch_encode_plus
    encodings = self._tokenizer.encode_batch(
TypeError: PreTokenizedEncodeInput must be Union[PreTokenizedInputSequence, Tuple[PreTokenizedInputSequence, PreTokenizedInputSequence]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/h_ghazik/plm_secondary_integration/ionchannel_embedding.py", line 170, in <module>
    fire.Fire(main)
  File "/home/h_ghazik/.conda/envs/py39/lib/python3.9/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/home/h_ghazik/.conda/envs/py39/lib/python3.9/site-packages/fire/core.py", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File "/home/h_ghazik/.conda/envs/py39/lib/python3.9/site-packages/fire/core.py", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "/home/h_ghazik/plm_secondary_integration/ionchannel_embedding.py", line 121, in main
    training_embeddings, training_labels = embed_dataset(
  File "/home/h_ghazik/plm_secondary_integration/ionchannel_embedding.py", line 97, in embed_dataset
    raise Exception(
Exception: Error processing sequence at index 0: MSHEKNEASGNPEAQSWKAQEAMLGVKTEVSRWRAVKNCLYRHLVKVLGEDWIFLLLLGALMALVSWAMDFIGSRGLRFYKYLFAMVEGNLGLQYLVWVCYPLILILFSSLFCQIVSPQAVGSGIPELKTIIRGAVLHEYLTLRTFVAKTVGLTVALSAGFPLGKEGPFVHIASICATLLNQLLCFISGRREEPYYLRADILTVGCALGISCCFGTPLAGVLFSIEVTCSHFGVRSYWRGFLGGAFSAFIFRVLSVWVKDTVTLTALFKTNFRGDIPFDLQELPAFAIIGIASGFFGALFVYLNRQIIVFMRKKNFVTKILKKQRLIYPAVVTFVLATLRFPPGVGQFFGAGLMPRETINSLFDNYTWTKTIDPRGLGNSAQWFIPHLNIFIVMALYFVMHFWMAALAVTMPVPCGAFVPVFNLGAVLGRFVGELMALLFPDGLVSNGNLYHILPGEYAVIGAAAMTGAVTHAVSTAVICFELTGQISHVLPMMVAVILANMVAQGLQPSLYDSIIQIKKLPYLPELSWSSANKYNIQVGDIMVRDVTSIASTSTYGDLLHVLRQTKLKFFPFVDTPETNTLLGSIERTEVEGLLQRRISAYRRQPATAAEAEEEGRNGERGASFTGDVPGEAETSFAYIDQEEAEGQQQREGLEAVKVQTEDPRPPSPVPAEEPTQTSGIYQKKHKGTGQVASRFEEMLTLEEIYQWEQREKNVVVNFETCRIDQSPFQLVEGTSLQKTHTLFSLLGLDRAYVTSMGKLVGVVALAEIQAAIEGSYQKGFRLPPPLASFRDAKNARNSGRTATSNSSGK - Error: PreTokenizedEncodeInput must be Union[PreTokenizedInputSequence, Tuple[PreTokenizedInputSequence, PreTokenizedInputSequence]]
