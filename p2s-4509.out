==========================================
SLURM_JOB_ID = 4509
SLURM_NODELIST = virya4
==========================================
/home/h_ghazik/.bash_profile: line 1: /media/Data/default/common/settings.sh: No such file or directory
Thu Oct 19 13:29:01 2023       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.86.10              Driver Version: 535.86.10    CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:01:00.0 Off |                   On |
| N/A   28C    P0              43W / 400W |     74MiB / 40960MiB |     N/A      Default |
|                                         |                      |              Enabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA A100-SXM4-40GB          Off | 00000000:41:00.0 Off |                   On |
| N/A   28C    P0              41W / 400W |     74MiB / 40960MiB |     N/A      Default |
|                                         |                      |              Enabled |
+-----------------------------------------+----------------------+----------------------+

+---------------------------------------------------------------------------------------+
| MIG devices:                                                                          |
+------------------+--------------------------------+-----------+-----------------------+
| GPU  GI  CI  MIG |                   Memory-Usage |        Vol|      Shared           |
|      ID  ID  Dev |                     BAR1-Usage | SM     Unc| CE ENC DEC OFA JPG    |
|                  |                                |        ECC|                       |
|==================+================================+===========+=======================|
|  0    1   0   0  |              37MiB / 19968MiB  | 42      0 |  3   0    2    0    0 |
|                  |               0MiB / 32767MiB  |           |                       |
+------------------+--------------------------------+-----------+-----------------------+
|  0    2   0   1  |              37MiB / 19968MiB  | 42      0 |  3   0    2    0    0 |
|                  |               0MiB / 32767MiB  |           |                       |
+------------------+--------------------------------+-----------+-----------------------+
|  1    1   0   0  |              37MiB / 19968MiB  | 42      0 |  3   0    2    0    0 |
|                  |               0MiB / 32767MiB  |           |                       |
+------------------+--------------------------------+-----------+-----------------------+
|  1    2   0   1  |              37MiB / 19968MiB  | 42      0 |  3   0    2    0    0 |
|                  |               0MiB / 32767MiB  |           |                       |
+------------------+--------------------------------+-----------+-----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
/usr/bin/bash: /usr/local/pkg/anaconda/v3.2023.03/root/lib/libtinfo.so.6: no version information available (required by /usr/bin/bash)
WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
wandb: Currently logged in as: hamed-ghazikhani (bioinformatics-group). Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/h_ghazik/.netrc
wandb: Currently logged in as: hamed-ghazikhani (bioinformatics-group). Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/h_ghazik/.netrc
wandb: Currently logged in as: hamed-ghazikhani (bioinformatics-group). Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/h_ghazik/.netrc
wandb: Currently logged in as: hamed-ghazikhani (bioinformatics-group). Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/h_ghazik/.netrc
Max length:  825
Max length:  825
Max length:  825
Max length:  825
Running tokenizer on dataset for validation:   0%|          | 0/1131 [00:00<?, ? examples/s]Running tokenizer on dataset for validation:   0%|          | 0/1131 [00:00<?, ? examples/s]Running tokenizer on dataset for validation:  88%|████████▊ | 1000/1131 [00:00<00:00, 2047.29 examples/s]Running tokenizer on dataset for validation: 100%|██████████| 1131/1131 [00:00<00:00, 1853.02 examples/s]
Running tokenizer on dataset for validation:  88%|████████▊ | 1000/1131 [00:00<00:00, 2144.60 examples/s]Running tokenizer on dataset for validation: 100%|██████████| 1131/1131 [00:00<00:00, 1943.29 examples/s]
Traceback (most recent call last):
  File "/home/h_ghazik/plm_secondary_integration/plm_secondary_accelerate.py", line 172, in <module>
    training_args = TrainingArguments(
  File "<string>", line 115, in __init__
  File "/home/h_ghazik/.conda/envs/py39/lib/python3.9/site-packages/transformers/training_args.py", line 1436, in __post_init__
    and (self.device.type != "cuda")
  File "/home/h_ghazik/.conda/envs/py39/lib/python3.9/site-packages/transformers/training_args.py", line 1901, in device
    return self._setup_devices
  File "/home/h_ghazik/.conda/envs/py39/lib/python3.9/site-packages/transformers/utils/generic.py", line 54, in __get__
    cached = self.fget(obj)
  File "/home/h_ghazik/.conda/envs/py39/lib/python3.9/site-packages/transformers/training_args.py", line 1831, in _setup_devices
    self.distributed_state = PartialState(
  File "/home/h_ghazik/.conda/envs/py39/lib/python3.9/site-packages/accelerate/state.py", line 208, in __init__
    torch.cuda.set_device(self.device)
  File "/home/h_ghazik/.conda/envs/py39/lib/python3.9/site-packages/torch/cuda/__init__.py", line 404, in set_device
    torch._C._cuda_setDevice(device)
RuntimeError: CUDA error: invalid device ordinal
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Traceback (most recent call last):
  File "/home/h_ghazik/plm_secondary_integration/plm_secondary_accelerate.py", line 172, in <module>
    training_args = TrainingArguments(
  File "<string>", line 115, in __init__
  File "/home/h_ghazik/.conda/envs/py39/lib/python3.9/site-packages/transformers/training_args.py", line 1436, in __post_init__
    and (self.device.type != "cuda")
  File "/home/h_ghazik/.conda/envs/py39/lib/python3.9/site-packages/transformers/training_args.py", line 1901, in device
    return self._setup_devices
  File "/home/h_ghazik/.conda/envs/py39/lib/python3.9/site-packages/transformers/utils/generic.py", line 54, in __get__
    cached = self.fget(obj)
  File "/home/h_ghazik/.conda/envs/py39/lib/python3.9/site-packages/transformers/training_args.py", line 1831, in _setup_devices
    self.distributed_state = PartialState(
  File "/home/h_ghazik/.conda/envs/py39/lib/python3.9/site-packages/accelerate/state.py", line 208, in __init__
    torch.cuda.set_device(self.device)
  File "/home/h_ghazik/.conda/envs/py39/lib/python3.9/site-packages/torch/cuda/__init__.py", line 404, in set_device
    torch._C._cuda_setDevice(device)
RuntimeError: CUDA error: invalid device ordinal
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Traceback (most recent call last):
  File "/home/h_ghazik/plm_secondary_integration/plm_secondary_accelerate.py", line 172, in <module>
    training_args = TrainingArguments(
  File "<string>", line 115, in __init__
  File "/home/h_ghazik/.conda/envs/py39/lib/python3.9/site-packages/transformers/training_args.py", line 1436, in __post_init__
    and (self.device.type != "cuda")
  File "/home/h_ghazik/.conda/envs/py39/lib/python3.9/site-packages/transformers/training_args.py", line 1901, in device
    return self._setup_devices
  File "/home/h_ghazik/.conda/envs/py39/lib/python3.9/site-packages/transformers/utils/generic.py", line 54, in __get__
    cached = self.fget(obj)
  File "/home/h_ghazik/.conda/envs/py39/lib/python3.9/site-packages/transformers/training_args.py", line 1831, in _setup_devices
    self.distributed_state = PartialState(
  File "/home/h_ghazik/.conda/envs/py39/lib/python3.9/site-packages/accelerate/state.py", line 208, in __init__
    torch.cuda.set_device(self.device)
  File "/home/h_ghazik/.conda/envs/py39/lib/python3.9/site-packages/torch/cuda/__init__.py", line 404, in set_device
    torch._C._cuda_setDevice(device)
RuntimeError: CUDA error: invalid device ordinal
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[2023-10-19 13:29:30,606] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1835550 closing signal SIGTERM
[2023-10-19 13:29:31,021] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 1 (pid: 1835551) of binary: /home/h_ghazik/.conda/envs/py39/bin/python
Traceback (most recent call last):
  File "/home/h_ghazik/.conda/envs/py39/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/h_ghazik/.conda/envs/py39/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/h_ghazik/.conda/envs/py39/lib/python3.9/site-packages/torch/distributed/run.py", line 810, in <module>
    main()
  File "/home/h_ghazik/.conda/envs/py39/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/home/h_ghazik/.conda/envs/py39/lib/python3.9/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/home/h_ghazik/.conda/envs/py39/lib/python3.9/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/home/h_ghazik/.conda/envs/py39/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/h_ghazik/.conda/envs/py39/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
plm_secondary_accelerate.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2023-10-19_13:29:30
  host      : virya4
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 1835552)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2023-10-19_13:29:30
  host      : virya4
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 1835553)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-10-19_13:29:30
  host      : virya4
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 1835551)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: virya4: task 0: Exited with exit code 1
